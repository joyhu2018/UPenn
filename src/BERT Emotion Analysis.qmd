---
title: "BERT Emotion Analysis"
format: html
---

## BERT Emotion Analysis

We use a BERT model to analyze emotion.

The emotion analysis data will tell us the most significant emotion from ANGER, SADNESS, and JOY and assign a numerical value between 0 and 1 to the most significant emotion only.

```{python}
from google.colab import drive
drive.mount('/content/drive')
```

```{python}
model.save_pretrained('/content/drive/My Drive/model')
tokenizer.save_pretrained('/content/drive/My Drive/tokenizer')
```

```{python}
model = AutoModelForSequenceClassification.from_pretrained('/content/drive/My Drive/model') tokenizer = AutoTokenizer.from_pretrained('/content/drive/My Drive/tokenizer')
```

```{python}
!pip install numpy torch datasets transformers~=4.28.0 evaluate tqdm --quiet
!pip freeze | grep -E '^numpy|^torch|^datasets|^transformers|^evaluate'
```

```{python}
from datasets import load_dataset
from transformers import AutoTokenizer
import evaluate
import numpy as np
from transformers import AutoModelForSequenceClassification
from transformers import Trainer, TrainingArguments
from transformers import pipeline
import torch


# https://huggingface.co/datasets/tweet_eval/viewer/emotion/train
raw_datasets = load_dataset('tweet_eval', 'emotion')
```

```{python}
import pandas as pd
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
import torch
raw_datasets['train']
```

```{python}
# let's print out 5 examples with respective labels
id2label = {0:'ANGER',1:'JOY',2:'OPTIMISM',3:'SADNESS'}
for i in range(5):
    print(f"TEXT[{i}]: {raw_datasets['train']['text'][i]}")
    print(f"LABEL[{i}]: {id2label[raw_datasets['train']['label'][i]]}")
    print()

# remove optimism as it's similar to joy
datasets = raw_datasets.filter(lambda row: row['label']!=2)
```

```{python}
# let's use the fast bert tokenizer
checkpoint = 'distilbert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True)

def tokenization(batch):
    tokenized_batch = tokenizer(batch['text'], truncation=True)
    # map SADNESS target label from 3 to 2
    tokenized_batch['label'] = [2 if label==3 else label for label in batch['label']]
    return tokenized_batch

# tokenize each text
tokenized_datasets = datasets.map(tokenization, batched=True, remove_columns=['text'])
# rename target variable 'label' to 'labels'
tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')
# set to pytorch
tokenized_datasets.set_format('torch')
```

```{python}
# load training metrices
accuracy_metric = evaluate.load('accuracy')
f1_metric = evaluate.load('f1')

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    accuracy = accuracy_metric.compute(predictions=preds, references=labels)
    # we use macro averaging to treat all classes equally to handle data imbalance
    f1 = f1_metric.compute(predictions=preds, references=labels, average='macro')
    return {**accuracy, **f1}

```

```{python}
# let's clone bert from: https://huggingface.co/distilbert-base-uncased
# setup as a multiclass classifier
checkpoint = 'distilbert-base-uncased'
num_labels = 3
id2label = {0:'ANGER',1:'JOY',2:'SADNESS'}
label2id = {'ANGER':0,'JOY':1,'SADNESS':2}
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels, id2label=id2label, label2id=label2id)

```

```{python}
# seed: for reproducibility
# output_dir: output directory to store model epoch checkpoints
# num_train_epochs: number of training epochs
# save_strategy,evaluation_strategy: evaluate for each epoch
# load_best_model_at_end: load model with lowest validation loss
# report_to: suppress third-party logging

training_args = TrainingArguments(
    seed=42,
    output_dir='results',
    num_train_epochs=3,
    save_strategy='epoch',
    evaluation_strategy='epoch',
    load_best_model_at_end=True,
    report_to='none',
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['validation'],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.evaluate(tokenized_datasets['test'])
trainer.train()
trainer.evaluate(tokenized_datasets['test'])

twitter_emotion_classifier = pipeline(task='text-classification', model=model, tokenizer=tokenizer)
```

Testing the model.

```{python}
df = pd.read_csv('test.csv')
text_data = df['text'].tolist()

predictions = twitter_emotion_classifier(text_data)

labels = [prediction['label'] for prediction in predictions]
scores = [prediction['score'] for prediction in predictions]

df['emotion_label'] = labels
df['emotion_score'] = scores

df.to_csv('test.csv', index=False)

print("Sentiment analysis completed and results saved to 'test.csv'")

```

Analyzing Biden's tweets.

```{python}
df = pd.read_csv('data/biden2020.csv')
text_data = df['text'].tolist()

predictions = twitter_emotion_classifier(text_data)

labels = [prediction['label'] for prediction in predictions]
scores = [prediction['score'] for prediction in predictions]

df['emotion_label'] = labels
df['emotion_score'] = scores

df.to_csv('biden2020.csv', index=False)

print("Sentiment analysis completed and results saved to 'biden2020.csv'")

```

Analyzing Trump's tweets.

```{python}
df = pd.read_csv('trump2020.csv')
text_data = df['text'].tolist()

predictions = twitter_emotion_classifier(text_data)

labels = [prediction['label'] for prediction in predictions]
scores = [prediction['score'] for prediction in predictions]

df['emotion_label'] = labels
df['emotion_score'] = scores

df.to_csv('trump2020.csv', index=False)

print("Sentiment analysis completed and results saved to 'trump2020.csv'")
```
